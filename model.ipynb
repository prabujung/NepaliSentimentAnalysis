{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15001, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/dataset.csv')\n",
    "df = df.loc[:15000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_col = df.iloc[1:, 0]\n",
    "second_col = df.iloc[1:, 1]\n",
    "second_col = second_col.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = open(\"./data/stopwords.txt\",\"r\",encoding=\"utf-8\")\n",
    "stop_words = stop_words_file.read()\n",
    "stop_words = stop_words.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning method\n",
    "def data_cleaning(string):\n",
    "    text = re.sub('\\,|\\@|\\-|\\\"|\\'| \\)|\\(|\\)| \\{| \\}| \\[| \\]|!|‘|’|“|”| \\:-|\\?|।|/|\\—|\\०|\\१|\\२|\\३|\\४|\\५|\\६|\\७|\\८|\\९|[0-9]', '', string)\n",
    "    return text\n",
    "\n",
    "def stop_word_remove(array_element):\n",
    "    array_element_set = set(array_element)\n",
    "    final_list = list(array_element_set.difference(stop_words))\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize():\n",
    "    data_with_split = []\n",
    "    for data in first_col:\n",
    "        return_string = data_cleaning(data)\n",
    "        each_docs = return_string.split(\" \")\n",
    "        string_after_remove_word=stop_word_remove(each_docs)\n",
    "        \n",
    "        data_with_split.append(each_docs)\n",
    "    return data_with_split  # it returns arr of each docs with spleted words\n",
    "\n",
    "\n",
    "\n",
    "corpus = tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = None\n",
    "        self.idf = None\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for document in corpus:\n",
    "            self.vocabulary.update(document)\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "\n",
    "    def get_tif_idf_info(self,words,sentence_feature):\n",
    "        tf_idf_info = {}\n",
    "        for word in words:\n",
    "            index = self.vocabulary.index(word)\n",
    "            tf_idf_info[word]=sentence_feature[0][index]\n",
    "        return tf_idf_info\n",
    "    \n",
    "    def fit_transform(self, corpus):\n",
    "        # Calculate IDF\n",
    "        idf = {}\n",
    "        N = len(corpus)\n",
    "        for term in self.vocabulary:\n",
    "            df = sum(1 for document in corpus if term in document)\n",
    "            idf[term] = math.log(N / (1 + df))\n",
    "\n",
    "        # Transform documents to TF-IDF representation\n",
    "        tfidf_matrix = np.zeros((len(corpus), len(self.vocabulary)))\n",
    "        for i, document in enumerate(corpus):\n",
    "            tf = Counter(document)\n",
    "            total_terms = len(document)\n",
    "            for j, term in enumerate(self.vocabulary):\n",
    "                if total_terms != 0:\n",
    "                    tfidf_matrix[i, j] = (tf.get(term, 0) / total_terms) * idf[term]\n",
    "                else:\n",
    "                    tfidf_matrix[i, j] = 0  # Set TF-IDF to 0 if total_terms is 0\n",
    "\n",
    "        self.idf = idf\n",
    "        return tfidf_matrix\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        tfidf_matrix = np.zeros((len(corpus), len(self.vocabulary)))\n",
    "        for i, document in enumerate(corpus):\n",
    "            tf = Counter(document)\n",
    "            total_terms = len(document)\n",
    "            for j, term in enumerate(self.vocabulary):\n",
    "                if total_terms != 0:\n",
    "                    tfidf_matrix[i, j] = (tf.get(term, 0) / total_terms) * self.idf.get(term, 0)\n",
    "                else:\n",
    "                    tfidf_matrix[i, j] = 0  # Set TF-IDF to 0 if total_terms is 0\n",
    "        return tfidf_matrix\n",
    "\n",
    "# Create TFIDFVectorizer instance\n",
    "tfidf_vectorizer = TFIDFVectorizer()\n",
    "\n",
    "# Fit and transform corpus\n",
    "features = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer_data = open(\"./webapp/model/vectorizer.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_vectorizer, vectorizer_data)\n",
    "vectorizer_data.close()\n",
    "\n",
    "with open('./webapp/model/vectorizer.pkl', 'rb') as tfidf:\n",
    "    vectorizer = pickle.load(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x=features\n",
    "y=second_col\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.2,random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()  \n",
    "TrainData = naive_bayes.fit(train_x, train_y)\n",
    "\n",
    "classifier_data = open(\"./webapp/model/classifier.pkl\", \"wb\")\n",
    "pickle.dump(naive_bayes, classifier_data)\n",
    "classifier_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./webapp/model/classifier.pkl', 'rb') as pickle_saved_data:\n",
    "    unpickled_data = pickle.load(pickle_saved_data)\n",
    "\n",
    "\n",
    "\n",
    "prediction = unpickled_data.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6925421940637064\n",
      "Recall: 0.696\n",
      "Accuracy: 0.696\n",
      "F1 Score: 0.672075479230363\n"
     ]
    }
   ],
   "source": [
    "def calculate_performance_metrics(true_labels, predicted_labels):\n",
    "    precision = metrics.precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = metrics.recall_score(true_labels, predicted_labels, average='weighted')\n",
    "    accuracy = metrics.accuracy_score(true_labels, predicted_labels)\n",
    "    f1_score = metrics.f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    return precision, recall, accuracy, f1_score\n",
    "\n",
    "# Example usage:\n",
    "precision, recall, accuracy, f1_score = calculate_performance_metrics(test_y, prediction)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'दुखी': 8.517193191416238}\n",
      "Predicted Sentiment Label: -1\n",
      "cleaned sentence:  म  दुखी छु\n",
      "Tokenized sentence:  ['म', 'दुखी', 'छु']\n",
      "Stop Word  removed sentence:  ['दुखी']\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    # Preprocess the input sentence\n",
    "    cleaned_sentence = data_cleaning(sentence)\n",
    "    tokenized_sentence = cleaned_sentence.split()\n",
    "    stop_word_removed_sentence = stop_word_remove(tokenized_sentence)\n",
    "    \n",
    "    # Transform the preprocessed sentence using TF-IDF vectorizer\n",
    "    sentence_features = vectorizer.transform([stop_word_removed_sentence])\n",
    "    print(vectorizer.get_tif_idf_info(stop_word_removed_sentence,sentence_features))\n",
    "\n",
    "    # Use the trained classifier to predict the sentiment label\n",
    "    predicted_label = unpickled_data.predict(sentence_features)\n",
    "\n",
    "    return cleaned_sentence,tokenized_sentence,stop_word_removed_sentence,predicted_label[0]  # Return the predicted sentiment label\n",
    "\n",
    "# Example usage:\n",
    "sentence = \"म  दुखी छु\"\n",
    "cleaned,tokenized,sw_removed,predicted_sentiment = predict_sentiment(sentence)\n",
    "print(\"Predicted Sentiment Label:\", predicted_sentiment)\n",
    "print(\"cleaned sentence: \", cleaned)\n",
    "print(\"Tokenized sentence: \",tokenized)\n",
    "print(\"Stop Word  removed sentence: \", sw_removed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
